#!/usr/bin/env python3
"""
LLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ë° ëª¨ë¸ ë¹„êµ
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd

try:
    import matplotlib.pyplot as plt
    import seaborn as sns

    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("matplotlib ë¯¸ì„¤ì¹˜")


class LLMBenchmarkAnalyzer:
    """LLM ë²¤ì¹˜ë§ˆí¬ ë¶„ì„"""

    def __init__(self, results_dir: str = "./benchmark_results"):
        self.results_dir = Path(results_dir)
        self.csv_files = list(self.results_dir.glob("llm_benchmark_*.csv"))
        self.json_files = list(self.results_dir.glob("llm_benchmark_*.json"))

        self.data = {}
        self.reports = {}
        self._load_all_results()

    def plot_comparison(self):
        """ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        if not HAS_MATPLOTLIB or len(self.reports) < 2:
            print("matplotlib í•„ìš”í•˜ê±°ë‚˜ ëª¨ë¸ ë¶€ì¡±")
            return

        # âœ… reports.keys() ì§ì ‘ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
        models = [m.replace("_", ":") for m in self.reports.keys()]
        accuracies = [
            self.reports[m].get("tool_accuracy", 0) for m in self.reports.keys()
        ]
        latencies = [
            self.reports[m].get("avg_latency_ms", 0) for m in self.reports.keys()
        ]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # ë„êµ¬ ì •í™•ë„
        colors1 = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models)))
        ax1.bar(models, accuracies, color=colors1)
        ax1.set_ylabel("ë„êµ¬ ì„ íƒ ì •í™•ë„ (%)")
        ax1.set_title("LLM ë„êµ¬ ì„ íƒ ì •í™•ë„ ë¹„êµ")
        ax1.set_ylim(0, 105)
        ax1.grid(axis="y", alpha=0.3)

        for i, acc in enumerate(accuracies):
            ax1.text(i, acc + 1, f"{acc:.1f}%", ha="center")

            # ì‘ë‹µ ì‹œê°„
            colors2 = plt.cm.RdYlGn_r(np.linspace(0.3, 0.9, len(models)))
            ax2.bar(models, latencies, color=colors2)
            ax2.set_ylabel("ì‘ë‹µ ì‹œê°„ (ms)")
            ax2.set_title("LLM ì‘ë‹µ ì‹œê°„ ë¹„êµ")
            ax2.grid(axis="y", alpha=0.3)

            for i, lat in enumerate(latencies):
                ax2.text(i, lat + 20, f"{lat:.0f}ms", ha="center")

            plt.tight_layout()
            output_file = (
                self.results_dir
                / f"llm_comparison_{datetime.now().strftime('%Y-%m-%d')}.png"
            )
            plt.savefig(output_file, dpi=150)
            print(f"âœ“ ì°¨íŠ¸ ì €ì¥: {output_file}")

    def _load_all_results(self):
        """ëª¨ë“  ê²°ê³¼ ë¡œë“œ"""
        print(f"\në¡œë“œ ì¤‘: {self.results_dir}\n")

        for json_file in self.json_files:
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    report = json.load(f)
                    model = report.get("model", "unknown")
                    self.reports[model] = report
                    print(f"âœ“ {json_file.name}")
            except Exception as e:
                print(f"âœ— {json_file.name}: {e}")

        for csv_file in self.csv_files:
            try:
                df = pd.read_csv(csv_file, encoding="utf-8")
                # ëª¨ë¸ëª… ì¶”ì¶œ
                filename = csv_file.stem  # llm_benchmark_2026-01-09_llama3.2:7b
                parts = filename.split("_")
                if len(parts) >= 4:
                    model = "_".join(parts[3:]).replace(":", "_")
                else:
                    model = "unknown"

                self.data[model] = df
                print(f"âœ“ {csv_file.name} ({len(df)} í…ŒìŠ¤íŠ¸)")
            except Exception as e:
                print(f"âœ— {csv_file.name}: {e}")

    def print_summary(self):
        """ì „ì²´ ìš”ì•½ ì¶œë ¥"""
        if not self.reports:
            print("ë°ì´í„° ì—†ìŒ")
            return

        print(f"\n{'=' * 90}")
        print(f"LLM ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print(f"{'=' * 90}\n")

        # ëª¨ë¸ë³„ ì •ë ¬
        sorted_models = sorted(
            self.reports.items(),
            key=lambda x: x[1].get("tool_accuracy", 0),
            reverse=True,
        )

        print(
            f"{'ëª¨ë¸':<20} {'ë„êµ¬ì •í™•ë„':<12} {'JSONìœ íš¨ìœ¨':<12} {'ì‘ë‹µì‹œê°„':<12} {'ì„±ê³µë¥ ':<10}"
        )
        print("-" * 90)

        for model, report in sorted_models:
            model_display = model.replace("_", ":")
            accuracy = report.get("tool_accuracy", 0)
            json_rate = report.get("json_valid_rate", 0)
            latency = report.get("avg_latency_ms", 0)
            success_rate = report.get("success_rate", 0)

            print(
                f"{model_display:<20} {accuracy:>10.1f}% {json_rate:>10.1f}% "
                f"{latency:>10.1f}ms {success_rate:>8.1f}%"
            )

    def print_detailed_analysis(self):
        """ìƒì„¸ ë¶„ì„ ì¶œë ¥"""
        for model_display, report in sorted(self.reports.items()):
            model_name = model_display.replace("_", ":")

            print(f"\n{'=' * 90}")
            print(f"ğŸ“Š {model_name}")
            print(f"{'=' * 90}\n")

            print(f"ì´ í…ŒìŠ¤íŠ¸: {report.get('total_tests')}")
            print(f"ë„êµ¬ ì •í™•ë„: {report.get('tool_accuracy', 0):.1f}%")
            print(f"JSON ìœ íš¨ìœ¨: {report.get('json_valid_rate', 0):.1f}%")
            print(f"ì„±ê³µë¥ : {report.get('success_rate', 0):.1f}%\n")

            print(f"ì‘ë‹µ ì‹œê°„ (ms):")
            print(f"  í‰ê· : {report.get('avg_latency_ms', 0):.1f}")
            print(f"  ì¤‘ì•™ê°’: {report.get('median_latency_ms', 0):.1f}")
            print(f"  P95: {report.get('p95_latency_ms', 0):.1f}")
            print(f"  P99: {report.get('p99_latency_ms', 0):.1f}")
            print(f"  ìµœì†Œ: {report.get('min_latency_ms', 0):.1f}")
            print(f"  ìµœëŒ€: {report.get('max_latency_ms', 0):.1f}")
            print(f"  í‘œì¤€í¸ì°¨: {report.get('std_dev_ms', 0):.1f}\n")

    def print_category_analysis(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
        print(f"\n{'=' * 90}")
        print(f"ì¹´í…Œê³ ë¦¬ë³„ ë„êµ¬ ì •í™•ë„ ë¶„ì„")
        print(f"{'=' * 90}\n")

        for model, df in self.data.items():
            model_name = model.replace("_", ":")
            print(f"\n[{model_name}]")

            if "category" not in df.columns or "tool_correct" not in df.columns:
                print("  ë°ì´í„° ë¶€ì¡±")
                continue

            categories = (
                df.groupby("category")
                .agg({"tool_correct": ["sum", "count", "mean"]})
                .round(3)
            )

            categories.columns = ["ì •í™•", "ì´ê°œ", "ì •í™•ë„"]
            categories["ì •í™•ë„"] = (categories["ì •í™•ë„"] * 100).round(1)

            for category, row in categories.iterrows():
                print(
                    f"  {category:25s}: "
                    f"{int(row['ì •í™•']):2d}/{int(row['ì´ê°œ']):2d} "
                    f"({row['ì •í™•ë„']:5.1f}%)"
                )

    def compare_models(self):
        """ëª¨ë¸ ê°„ ë¹„êµ"""
        if len(self.reports) < 2:
            print("ë¹„êµí•  ëª¨ë¸ ë¶€ì¡± (2ê°œ ì´ìƒ í•„ìš”)")
            return

        print(f"\n{'=' * 90}")
        print(f"ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„")
        print(f"{'=' * 90}\n")

        # ë„êµ¬ ì •í™•ë„ ìˆœìœ„
        print("ë„êµ¬ ì„ íƒ ì •í™•ë„")
        models_by_accuracy = sorted(
            self.reports.items(),
            key=lambda x: x[1].get("tool_accuracy", 0),
            reverse=True,
        )
        for rank, (model, report) in enumerate(models_by_accuracy, 1):
            model_name = model.replace("_", ":")
            accuracy = report.get("tool_accuracy", 0)
            print(f"   {rank}. {model_name:20s}: {accuracy:6.1f}%")

        # ì‘ë‹µ ì†ë„ ìˆœìœ„
        print("\nì‘ë‹µ ì†ë„ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)")
        models_by_speed = sorted(
            self.reports.items(), key=lambda x: x[1].get("avg_latency_ms", float("inf"))
        )
        for rank, (model, report) in enumerate(models_by_speed, 1):
            model_name = model.replace("_", ":")
            latency = report.get("avg_latency_ms", 0)
            print(f"   {rank}. {model_name:20s}: {latency:7.1f}ms")

        # ì¢…í•© ì ìˆ˜
        print("\nì¢…í•© ì ìˆ˜ (ìµœëŒ€ 100ì )")
        scores = {}
        for model, report in self.reports.items():
            accuracy_score = (report.get("tool_accuracy", 0) / 100) * 40
            json_score = (report.get("json_valid_rate", 0) / 100) * 20
            latency = report.get("avg_latency_ms", 1000)
            speed_score = max(0, (1 - latency / 2000) * 40)  # 2000ms ê¸°ì¤€

            total_score = accuracy_score + json_score + speed_score
            scores[model] = total_score

        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        for rank, (model, score) in enumerate(sorted_scores, 1):
            model_name = model.replace("_", ":")
            print(f"   {rank}. {model_name:20s}: {score:6.1f}/100")

    def export_comparison_table(self):
        """ë¹„êµ í…Œì´ë¸” ë‚´ë³´ë‚´ê¸°"""
        if not self.reports:
            print("ë°ì´í„° ì—†ìŒ")
            return

        comparison = []
        for model, report in self.reports.items():
            comparison.append(
                {
                    "ëª¨ë¸": model.replace("_", ":"),
                    "ì´í…ŒìŠ¤íŠ¸": report.get("total_tests"),
                    "ë„êµ¬ì •í™•ë„": f"{report.get('tool_accuracy', 0):.1f}%",
                    "JSONìœ íš¨ìœ¨": f"{report.get('json_valid_rate', 0):.1f}%",
                    "í‰ê· ì‘ë‹µì‹œê°„": f"{report.get('avg_latency_ms', 0):.1f}ms",
                    "ì¤‘ì•™ê°’": f"{report.get('median_latency_ms', 0):.1f}ms",
                    "P95": f"{report.get('p95_latency_ms', 0):.1f}ms",
                    "ì„±ê³µë¥ ": f"{report.get('success_rate', 0):.1f}%",
                }
            )

        df_comparison = pd.DataFrame(comparison)

        # ì½˜ì†”ì— ì¶œë ¥
        print("\n" + "=" * 100)
        print("ë¹„êµ í…Œì´ë¸”")
        print("=" * 100 + "\n")
        print(df_comparison.to_string(index=False))

        # CSVë¡œ ì €ì¥
        output_file = (
            self.results_dir
            / f"model_comparison_{datetime.now().strftime('%Y-%m-%d')}.csv"
        )
        df_comparison.to_csv(output_file, index=False, encoding="utf-8")
        print(f"\nâœ“ ì €ì¥: {output_file}\n")

        def plot_comparison(self):
            """ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
            if not HAS_MATPLOTLIB or len(self.reports) < 2:
                print("matplotlib í•„ìš”í•˜ê±°ë‚˜ ëª¨ë¸ ë¶€ì¡±")
                return

            # reports.keys()ë¥¼ ì§ì ‘ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
            models = [m.replace("_", ":") for m in self.reports.keys()]
            accuracies = [
                self.reports[m].get("tool_accuracy", 0) for m in self.reports.keys()
            ]
            latencies = [
                self.reports[m].get("avg_latency_ms", 0) for m in self.reports.keys()
            ]

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

            # ë„êµ¬ ì •í™•ë„
            colors1 = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models)))
            ax1.bar(models, accuracies, color=colors1)
            ax1.set_ylabel("ë„êµ¬ ì„ íƒ ì •í™•ë„ (%)")
            ax1.set_title("LLM ë„êµ¬ ì„ íƒ ì •í™•ë„ ë¹„êµ")
            ax1.set_ylim(0, 105)
            ax1.grid(axis="y", alpha=0.3)

            for i, acc in enumerate(accuracies):
                ax1.text(i, acc + 1, f"{acc:.1f}%", ha="center")

            # ì‘ë‹µ ì‹œê°„
            colors2 = plt.cm.RdYlGn_r(np.linspace(0.3, 0.9, len(models)))
            ax2.bar(models, latencies, color=colors2)
            ax2.set_ylabel("ì‘ë‹µ ì‹œê°„ (ms)")
            ax2.set_title("LLM ì‘ë‹µ ì‹œê°„ ë¹„êµ")
            ax2.grid(axis="y", alpha=0.3)

            for i, lat in enumerate(latencies):
                ax2.text(i, lat + 20, f"{lat:.0f}ms", ha="center")

            plt.tight_layout()
            output_file = (
                self.results_dir
                / f"llm_comparison_{datetime.now().strftime('%Y-%m-%d')}.png"
            )
            plt.savefig(output_file, dpi=150)
            print(f"âœ“ ì°¨íŠ¸ ì €ì¥: {output_file}")

    def generate_report(self):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸"""
        print(f"\n{'=' * 90}")
        print(f"LLM ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ë¦¬í¬íŠ¸")
        print(f"{'=' * 90}\n")

        self.print_summary()
        self.print_category_analysis()
        self.compare_models()
        self.export_comparison_table()
        self.plot_comparison()

        print(f"\n{'=' * 90}")
        print(f"ë¶„ì„ ì™„ë£Œ")
        print(f"{'=' * 90}\n")


def main():
    analyzer = LLMBenchmarkAnalyzer()
    analyzer.generate_report()
    analyzer.print_detailed_analysis()


if __name__ == "__main__":
    main()
